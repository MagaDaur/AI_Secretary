{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eresque/anaconda3/envs/urfo_hack_2024/lib/python3.12/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/eresque/anaconda3/envs/urfo_hack_2024/lib/python3.12/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import torch\n",
    "from whisperx.diarize import DiarizationPipeline\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_file, model_name=\"large-v3\", compute_type=\"float16\"):\n",
    "    \"\"\"Транскрибация аудиофайла\"\"\"\n",
    "    model = whisperx.load_model(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", compute_type=compute_type)\n",
    "    transcription_result = model.transcribe(audio_file)\n",
    "    return transcription_result\n",
    "\n",
    "def perform_diarization(audio_file):\n",
    "    \"\"\"Диаризация аудиофайла и выделение самого длинного сегмента используя WhisperX\"\"\"\n",
    "    diarization_pipeline = DiarizationPipeline(use_auth_token=HF_TOKEN, device='cuda')\n",
    "    diarized = diarization_pipeline(audio_file)\n",
    "\n",
    "    return diarized\n",
    "\n",
    "def count_unique_speakers(diarize_df):\n",
    "    \"\"\"Подсчёт уникальных спикеров в результатах диаризации\"\"\"\n",
    "    unique_speakers = diarize_df['speaker'].nunique()\n",
    "    return unique_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестовая обработка аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a769017175487aabab6f89c26a26cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd1bd39ef054298920e9c683cabca74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae767f33154044fbb6d75bd126d25ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf8a792b3fa4b5e8d10d5dc23ae83fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876fcc067af64970a1a185d1274a6444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: ru (0.98) in first 30s of audio...\n",
      "Transcription time: 281.39105129241943\n",
      "Diarization time: 2.883897304534912\n",
      "Transcribed text:  Вы знаете, что в Государственную Думу у меня вынесено предложение о назначении вас на должность председателя правительства Российской Федерации. Совсем недавно мы встречались с коллегами и оценивали работу правительства за предыдущие годы. Сделано в сложных условиях немало, и мне кажется, что было бы правильно, если бы  Мы продолжили с вами работу, и вы продолжили работу в качестве председателя правительства.  Мы с вами говорили и о структуре, говорили о персонале. В целом, думаю, мы на правильном пути. И очень надеюсь на то, что депутаты Государственной Думы, а вы не так давно были в Госдуме, отчитывались, они знают, что и как правительство, и вами, как председателям правительства, сделано за последние годы, оценят должным образом и поддержат вас в ходе ваших консультаций, предстоящих сегодня в  На фракциях, а затем и на заседании парламента. После того, как это произойдёт, а надеюсь, что это так и будет, мы с вами встретимся ещё раз и поговорим более подробно тогда о ваших предложениях. В соответствии с Конституцией будем действовать. Надеюсь, что вам удастся убедить депутатов в Государственной Думе по поводу кандидатуры ваших заместителей и федеральных министров.  Спасибо, уважаемый Владимир Владимирович. Хочу в первую очередь поблагодарить вас за доверие, которое вы оказали мне, за задачи, которые вы поставили перед Федеральным собранием в своем послании и, конечно, те национальные цели развития, которые были указаны в новом майском указе.  Это ориентир и приоритеты в работе правительства. Хочу вас заверить, что никаких пауз в работе правительства не будет. Мы будем продолжать текущую работу. Также считаю, что мы должны обеспечить преемственность по всем национальным целям, которые были до этого, в 204 и 474 указе. Сделаем все для развития нашей экономики, чтобы оправдать доверие наших людей. И уверен, что под вашим руководством мы все задачи, которые поставлены, решим.  Мы с вами вместе и с коллегами из правительства формулировали национальные цели развития. Это, конечно, главное, к чему мы должны стремиться, к реализации этих целей по всем направлениям. И, как показывает практика последних лет, в целом у нас…  И получается добиваться тех результатов, которые нужны стране. А в сегодняшних непростых условиях, конечно, нужно собраться и нужно организовать работу именно так, как мы с вами договорились на последней встрече с правительством, работать без пауз.\n",
      "Number of speakers: 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def base(audio_file):\n",
    "    start_time1 = time.time()\n",
    "    transcription_result = transcribe_audio(audio_file)\n",
    "    end_time1 = time.time()\n",
    "    processing_time = end_time1 - start_time1\n",
    "    print(f'Transcription time: {processing_time}')\n",
    "\n",
    "    \n",
    "\n",
    "    # Measure diarization time\n",
    "    start_time = time.time()\n",
    "    diarize_df = perform_diarization(audio_file)\n",
    "    end_time = time.time()\n",
    "    processing_time1 = end_time - start_time\n",
    "    print(f'Diarization time: {processing_time1}')\n",
    "\n",
    "    # Count unique speakers\n",
    "    num_speakers = count_unique_speakers(diarize_df)\n",
    "\n",
    "    # Extract text from transcription result\n",
    "    if isinstance(transcription_result, list):\n",
    "        text = \" \".join([item['text'] for item in transcription_result])\n",
    "    elif isinstance(transcription_result, dict) and 'segments' in transcription_result:\n",
    "        text = \" \".join([segment['text'] for segment in transcription_result['segments']])\n",
    "    else:\n",
    "        text = transcription_result.get('text', '')\n",
    "\n",
    "    return text, num_speakers\n",
    "\n",
    "# Example usage\n",
    "text, num_speakers = base('аудио/Встреча 8. .m4a')\n",
    "print(f'Transcribed text: {text}')\n",
    "print(f'Number of speakers: {num_speakers}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация submission с временем работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: ru (1.00) in first 30s of audio...\n",
      "Transcription time: 32.58378529548645\n",
      "Diarization time: 10.450380086898804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: ru (1.00) in first 30s of audio...\n",
      "Transcription time: 25.41658878326416\n",
      "Diarization time: 7.72376012802124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: ru (0.99) in first 30s of audio...\n",
      "Transcription time: 33.45371389389038\n",
      "Diarization time: 10.222927570343018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: ru (1.00) in first 30s of audio...\n",
      "Transcription time: 13.42027997970581\n",
      "Diarization time: 4.556212425231934\n",
      "Dataset has been created and saved to 'dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_dataset_from_audio_folder(folder_path):\n",
    "    # Список для хранения данных\n",
    "    data = []\n",
    "\n",
    "    # Перебор всех файлов в папке\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".wav\") or file_name.endswith(\".mp3\") or file_name.endswith(\".m4a\") or file_name.endswith(\".ogg\"):  # Учитываем только аудиофайлы\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Получаем транскрибированный текст и число спикеров\n",
    "            try:\n",
    "                transcribed_text, number_of_speakers = base(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                transcribed_text, number_of_speakers = \"Ошибка при обработке\", 0\n",
    "            \n",
    "            # Добавляем данные в список\n",
    "            data.append({\n",
    "                'Наименование аудиозаписи': file_name,\n",
    "                'Транскрибированный текст': transcribed_text,\n",
    "                'Число спикеров': number_of_speakers\n",
    "            })\n",
    "\n",
    "    # Создаем DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Сохраняем DataFrame в CSV файл\n",
    "    df.to_csv('dataset.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"Dataset has been created and saved to 'dataset.csv'\")\n",
    "\n",
    "# Пример использования\n",
    "folder_path = 'audio_test/'\n",
    "create_dataset_from_audio_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"submission.csv\")\n",
    "dataset.to_csv(\"submission.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urfo_2024",
   "language": "python",
   "name": "urfo_2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
